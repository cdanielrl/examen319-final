22/12/09 22:32:06 WARN Utils: Your hostname, inf319 resolves to a loopback address: 127.0.1.1; using 192.168.4.210 instead (on interface ens33)
22/12/09 22:32:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/12/09 22:32:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://192.168.4.210:4040
Spark context available as 'sc' (master = local[*], app id = local-1670639550749).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.1
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 17.0.4)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.sql.SparkSession                         // Import de librería necesario
import org.apache.spark.sql.SparkSession

scala> import org.apache.spark.sql.Dataset                              // Import de librería necesario
import org.apache.spark.sql.Dataset

scala> import org.apache.spark.sql.DataFrame                            // Import de librería necesario
import org.apache.spark.sql.DataFrame

scala> val spark: SparkSession = SparkSession.builder().master("local[*]").appName("simple-app").getOrCreate()    // Crea una nueva sesión de Spark en la instancia local con el nombre "simple-app"
22/12/09 22:33:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6cf0fb4c

scala> val dataSet: Dataset[String] = spark.read.textFile("textfile.csv")     // Abre el archivo "textfile.csv" dentro de la sesion de Spark creada y lo carga en un objeto DataSet
dataSet: org.apache.spark.sql.Dataset[String] = [value: string]

scala> val df: DataFrame = dataSet.toDF()                                     // Convierte el dataset a un DataFrame y lo guarda en el objeto df
df: org.apache.spark.sql.DataFrame = [value: string]

scala> df.show()                                                              // Podemos ver el contenido del DataFrame
+---------------+                                                               
|          value|
+---------------+
|5.1,3.5,1.4,0.2|
|4.9,3.0,1.4,0.2|
|4.7,3.2,1.3,0.2|
|4.6,3.1,1.5,0.2|
|5.0,3.6,1.4,0.2|
|5.4,3.9,1.7,0.4|
|4.6,3.4,1.4,0.3|
|5.0,3.4,1.5,0.2|
|4.4,2.9,1.4,0.2|
|4.9,3.1,1.5,0.1|
|5.4,3.7,1.5,0.2|
|4.8,3.4,1.6,0.2|
|4.8,3.0,1.4,0.1|
|4.3,3.0,1.1,0.1|
|5.8,4.0,1.2,0.2|
|5.7,4.4,1.5,0.4|
|5.4,3.9,1.3,0.4|
|5.1,3.5,1.4,0.3|
|5.7,3.8,1.7,0.3|
|5.1,3.8,1.5,0.3|
+---------------+
only showing top 20 rows


scala> 
